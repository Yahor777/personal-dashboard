"""
Main Processor - –ì–ª–∞–≤–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä OLX Scraper

–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª:
- –ö–æ–æ—Ä–¥–∏–Ω–∏—Ä—É–µ—Ç —Ä–∞–±–æ—Ç—É scraper –∏ llm_bridge
- –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ webhook
- –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ—à–∏–±–∫–∏ –∏ retry
- –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞
"""

import asyncio
import json
from typing import List, Dict, Any
from datetime import datetime
import requests
from playwright.async_api import async_playwright
from loguru import logger

from .utils import (
    load_config,
    setup_logging,
    load_user_agents
)
from .scraper import OLXScraper
from .llm_bridge import LlamaBridge, compute_simple_rating


class OLXProcessor:
    """
    –ì–ª–∞–≤–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä - –∫–æ–æ—Ä–¥–∏–Ω–∏—Ä—É–µ—Ç –≤–µ—Å—å pipeline
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        Args:
            config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏–∑ utils.load_config()
        """
        self.config = config
        self.webhook_url = config['webhook_url']
        self.webhook_timeout = config['webhook_timeout']
        self.webhook_retries = config['webhook_retries']
        
        logger.info("OLXProcessor initialized")
    
    async def process_all_queries(self):
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
        """
        queries = [q.strip() for q in self.config['search_queries'] if q.strip()]
        max_ads = self.config['max_ads']
        
        if not queries:
            logger.error("No search queries specified in config")
            return
        
        logger.info(f"Processing {len(queries)} search queries, max {max_ads} ads each")
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º Playwright
        async with async_playwright() as playwright:
            # –ó–∞–ø—É—Å–∫–∞–µ–º –±—Ä–∞—É–∑–µ—Ä
            browser_type = self.config['browser_type']
            headless = self.config['headless']
            
            if browser_type == 'chromium':
                browser = await playwright.chromium.launch(headless=headless)
            elif browser_type == 'firefox':
                browser = await playwright.firefox.launch(headless=headless)
            else:
                browser = await playwright.webkit.launch(headless=headless)
            
            try:
                # –ó–∞–≥—Ä—É–∂–∞–µ–º User-Agents
                user_agents = load_user_agents(self.config['user_agents_file'])
                
                # –°–æ–∑–¥–∞—ë–º scraper
                scraper = OLXScraper(self.config, user_agents)
                await scraper._init_browser()
                
                # –°–æ–∑–¥–∞—ë–º Llama bridge
                llama_bridge = LlamaBridge(self.config, browser)
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –∑–∞–ø—Ä–æ—Å
                for query in queries:
                    logger.info(f"\n{'='*60}")
                    logger.info(f"Processing query: '{query}'")
                    logger.info(f"{'='*60}\n")
                    
                    await self._process_single_query(
                        query,
                        max_ads,
                        scraper,
                        llama_bridge
                    )
                
                # –ó–∞–∫—Ä—ã–≤–∞–µ–º scraper
                await scraper._close_browser()
                
            finally:
                await browser.close()
        
        logger.info("\n‚úÖ All queries processed successfully!")
    
    async def _process_single_query(
        self,
        query: str,
        max_ads: int,
        scraper: OLXScraper,
        llama_bridge: LlamaBridge
    ):
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            max_ads: –ú–∞–∫—Å–∏–º—É–º –æ–±—ä—è–≤–ª–µ–Ω–∏–π
            scraper: OLXScraper instance
            llama_bridge: LlamaBridge instance
        """
        try:
            # –°–∫—Ä–∞–ø–∏–º –æ–±—ä—è–≤–ª–µ–Ω–∏—è
            ads = await scraper.scrape_search_query(query, max_ads)
            
            if not ads:
                logger.warning(f"No ads found for query: '{query}'")
                return
            
            logger.info(f"Found {len(ads)} ads for query '{query}', starting analysis...")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥–æ–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ
            for i, ad_data in enumerate(ads, 1):
                logger.info(f"\n--- Processing ad {i}/{len(ads)} ---")
                logger.info(f"Title: {ad_data.get('title', 'Unknown')[:50]}...")
                
                try:
                    # AI –∞–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ Llama
                    ai_analysis = await llama_bridge.analyze_ad(ad_data)
                    
                    # –í—ã—á–∏—Å–ª—è–µ–º fallback —Ä–µ–π—Ç–∏–Ω–≥
                    fallback_score = compute_simple_rating(ad_data)
                    
                    # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—ã–π payload
                    payload = self._build_payload(ad_data, ai_analysis, fallback_score)
                    
                    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –Ω–∞ webhook
                    success = await self._send_to_webhook(payload)
                    
                    if success:
                        logger.info(f"‚úÖ Ad {i} processed and sent successfully")
                    else:
                        logger.error(f"‚ùå Failed to send ad {i} to webhook")
                    
                    # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –æ–±—ä—è–≤–ª–µ–Ω–∏—è–º–∏
                    await asyncio.sleep(1)
                    
                except Exception as e:
                    logger.error(f"Error processing ad {i}: {e}")
                    continue
            
            logger.info(f"\n‚úÖ Query '{query}' completed: {len(ads)} ads processed")
            
        except Exception as e:
            logger.error(f"Error processing query '{query}': {e}")
    
    def _build_payload(
        self,
        ad_data: Dict[str, Any],
        ai_analysis: Dict[str, Any],
        fallback_score: int
    ) -> Dict[str, Any]:
        """
        –§–æ—Ä–º–∏—Ä—É–µ—Ç –∏—Ç–æ–≥–æ–≤—ã–π payload –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –Ω–∞ webhook
        
        Args:
            ad_data: –î–∞–Ω–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è –æ—Ç scraper
            ai_analysis: AI –∞–Ω–∞–ª–∏–∑ –æ—Ç Llama
            fallback_score: –≠–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–π —Ä–µ–π—Ç–∏–Ω–≥
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –ø–æ–ª–Ω—ã–º –Ω–∞–±–æ—Ä–æ–º –¥–∞–Ω–Ω—ã—Ö
        """
        payload = {
            # –û—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            'url': ad_data.get('url'),
            'id': ad_data.get('id'),
            'title': ad_data.get('title'),
            'price': ad_data.get('price'),
            'currency': ad_data.get('currency'),
            'description': ad_data.get('description'),
            'images': ad_data.get('images', []),
            'date': ad_data.get('date'),
            'location': ad_data.get('location'),
            
            # AI –∞–Ω–∞–ª–∏–∑
            'ai_analysis': {
                'summary': ai_analysis.get('summary'),
                'selling_points': ai_analysis.get('selling_points', []),
                'score': ai_analysis.get('score'),
                'reasoning': ai_analysis.get('reasoning'),
                'missing_fields': ai_analysis.get('missing_fields', []),
                'is_fallback': ai_analysis.get('fallback', False)
            },
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            'fallback_score': fallback_score,
            
            # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            'metadata': {
                'scraped_at': ad_data.get('scraped_at'),
                'source': 'olx.pl',
                'search_query': ad_data.get('search_query'),
                'processor_version': '1.0.0'
            }
        }
        
        return payload
    
    async def _send_to_webhook(self, payload: Dict[str, Any]) -> bool:
        """
        –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –Ω–∞ webhook —Å retry –º–µ—Ö–∞–Ω–∏–∑–º–æ–º
        
        Args:
            payload: –î–∞–Ω–Ω—ã–µ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏
            
        Returns:
            True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ, False –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        for attempt in range(1, self.webhook_retries + 1):
            try:
                logger.debug(f"Sending to webhook (attempt {attempt}/{self.webhook_retries}): {self.webhook_url}")
                
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º POST –∑–∞–ø—Ä–æ—Å
                response = requests.post(
                    self.webhook_url,
                    json=payload,
                    timeout=self.webhook_timeout,
                    headers={
                        'Content-Type': 'application/json',
                        'User-Agent': 'OLX-Scraper/1.0'
                    }
                )
                
                response.raise_for_status()
                
                logger.info(f"Webhook response: {response.status_code}")
                logger.debug(f"Response body: {response.text[:200]}")
                
                return True
                
            except requests.exceptions.Timeout:
                logger.warning(f"Webhook timeout (attempt {attempt})")
                
                if attempt < self.webhook_retries:
                    await asyncio.sleep(2 ** attempt)  # Exponential backoff
                
            except requests.exceptions.RequestException as e:
                logger.error(f"Webhook request error (attempt {attempt}): {e}")
                
                if attempt < self.webhook_retries:
                    await asyncio.sleep(2 ** attempt)
                
            except Exception as e:
                logger.error(f"Unexpected webhook error (attempt {attempt}): {e}")
                break
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º payload –ª–æ–∫–∞–ª—å–Ω–æ –µ—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å
        self._save_failed_payload(payload)
        
        return False
    
    def _save_failed_payload(self, payload: Dict[str, Any]):
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç payload –ª–æ–∫–∞–ª—å–Ω–æ –µ—Å–ª–∏ webhook –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
        
        Args:
            payload: –î–∞–Ω–Ω—ã–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        """
        import os
        from pathlib import Path
        
        failed_dir = './data/failed_webhooks'
        Path(failed_dir).mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"failed_{timestamp}_{payload.get('id', 'unknown')}.json"
        filepath = os.path.join(failed_dir, filename)
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(payload, f, indent=2, ensure_ascii=False)
            
            logger.info(f"Failed payload saved locally: {filepath}")
            
        except Exception as e:
            logger.error(f"Could not save failed payload: {e}")


# ========================================
# –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
# ========================================

async def main():
    """
    –¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ –≤ –ø—Ä–æ–≥—Ä–∞–º–º—É
    """
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        logger.info("Loading configuration...")
        config = load_config()
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        setup_logging(
            log_level=config['log_level'],
            log_file=config['log_file'],
            console=config['log_console']
        )
        
        logger.info("="*60)
        logger.info("üöÄ OLX Scraper + Llama 4 AI Integration")
        logger.info("="*60)
        logger.info(f"Search queries: {', '.join(config['search_queries'])}")
        logger.info(f"Max ads per query: {config['max_ads']}")
        logger.info(f"Webhook URL: {config['webhook_url']}")
        logger.info(f"Llama URL: {config['llama_url']}")
        logger.info(f"Headless mode: {config['headless']}")
        logger.info("="*60 + "\n")
        
        # –°–æ–∑–¥–∞—ë–º –∏ –∑–∞–ø—É—Å–∫–∞–µ–º processor
        processor = OLXProcessor(config)
        await processor.process_all_queries()
        
        logger.info("\n" + "="*60)
        logger.info("‚úÖ Processing completed successfully!")
        logger.info("="*60)
        
    except KeyboardInterrupt:
        logger.warning("\n‚ö†Ô∏è Interrupted by user")
        
    except Exception as e:
        logger.error(f"\n‚ùå Fatal error: {e}")
        import traceback
        logger.error(traceback.format_exc())
        raise


if __name__ == '__main__':
    # –ó–∞–ø—É—Å–∫–∞–µ–º main —á–µ—Ä–µ–∑ asyncio
    asyncio.run(main())
